import os
import psutil
import time
import pandas as pd
from tqdm import tqdm
from datasets import load_dataset
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import spacy
import re

# ================================
# 1ï¸âƒ£ åŸºæœ¬è·¯å¾„é…ç½®
# ================================
DATA_DIR = "/mnt/d/idol01/homework/paper_code/tess-diffusion/processed_data/openwebtext_50/train"
OUTPUT_DIR = "/mnt/d/idol01/homework/paper_code/tess-diffusion/four"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ================================
# 2ï¸âƒ£ æ¨¡å‹å ä½ç¬¦ (å°†åœ¨æ¯ä¸ªå·¥ä½œè¿›ç¨‹ä¸­åˆå§‹åŒ–)
# ================================
rebel = None
rebel_model = None
rebel_tok = None
tokenizer = None
nlp = None

def init_worker():
    """åœ¨æ¯ä¸ªå·¥ä½œè¿›ç¨‹ä¸­ç‹¬ç«‹åˆå§‹åŒ–æ¨¡å‹ï¼Œé¿å…å¤šè¿›ç¨‹å†²çªã€‚"""
    global rebel, tokenizer, nlp, rebel_model, rebel_tok  # æå‰å£°æ˜æ‰€æœ‰å°†è¢«èµ‹å€¼çš„å…¨å±€å˜é‡

    # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé˜²æ­¢é‡å¤åŠ è½½
    if rebel_model is None or rebel_tok is None or tokenizer is None or nlp is None:
        print(f"ğŸš€ Initializing models in worker process: {os.getpid()}")
        # RoBERTa è§£ç å™¨ï¼ˆç”¨äºæŠŠ input_ids -> textï¼‰
        tokenizer = AutoTokenizer.from_pretrained("roberta-large")

        # REBEL åŸç”Ÿæ¨¡å‹ä¸åˆ†è¯å™¨ï¼ˆä¿ç•™ç‰¹æ®Š tokenï¼‰
        model_name = "Babelscape/rebel-large"
        rebel_tok = AutoTokenizer.from_pretrained(model_name)
        rebel_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        rebel_model.eval()

        # spaCy
        nlp = spacy.load("en_core_web_sm")

        # é€‰æ‹©è®¾å¤‡
        device = "cpu"
        rebel_model.to(device)

# ================================
# 3ï¸âƒ£ æ—¶é—´æå–è¾…åŠ©å‡½æ•°
# ================================
def extract_time_expressions(text):
    doc = nlp(text)
    times = [ent.text for ent in doc.ents if ent.label_ in ["DATE", "TIME"]]
    regex_times = re.findall(r"\b(?:\d{4}|\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*\d{0,2},?\s*\d{0,4})\b", text)
    times.extend(regex_times)
    return list(set(times)) if times else ["N/A"]

# ================================
# 3.1ï¸âƒ£ è§£æ REBEL ç”Ÿæˆç»“æœçš„å·¥å…·å‡½æ•°ï¼ˆé€‚é…å¤šç§æ ¼å¼ï¼‰
# ================================
def parse_rebel_output(generated_text: str):
    """å°† REBEL çš„ç”Ÿæˆæ–‡æœ¬è§£æä¸º [(head, relation, tail), ...]ã€‚
    å…¼å®¹ä»¥ä¸‹å‡ ç§å¸¸è§æ ¼å¼ï¼š
    - å¸¦ç‰¹æ®Šæ ‡è®°ï¼š<triplet> subject <subj> relation <rel> object <obj>
    - å¸¦åˆ†éš”ç¬¦ï¼š<triplet> subject <sep> relation <sep> object </triplet>
    - æ—§ç‰ˆæ‹¬å·æ ¼å¼ï¼š(head, relation, tail)
    - å…œåº•ï¼šæ ¹æ®åŒç©ºæ ¼å¯å‘å¼åˆ†å—ï¼ŒæŒ‰ (ent, rel, ent) ä¸‰å…ƒç»„æ»‘çª—æå–ï¼ˆå¼±ä¿è¯ï¼‰
    """
    triples = []
    s = generated_text or ""
    s = s.strip()
    if not s:
        return triples

    # ä¼˜å…ˆï¼š<triplet> ... è§£æ
    if "<triplet>" in s:
        parts = s.split("<triplet>")
        for p in parts[1:]:
            # æˆªæ–­è‡³ </triplet>ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "</triplet>" in p:
                p = p.split("</triplet>")[0]
            # å»æ‰å¥ç•Œæ ‡è®°
            if "</s>" in p:
                p = p.split("</s>")[0]
            p = p.replace("<s>", " ").strip()
            # ä¸‰ç§æ ‡è®°å°è¯•
            if all(tag in p for tag in ["<subj>", "<rel>", "<obj>"]):
                try:
                    i_sub = p.index("<subj>")
                    i_rel = p.index("<rel>")
                    i_obj = p.index("<obj>")
                    head = p[:i_sub].strip()
                    rel = p[i_sub + len("<subj>"): i_rel].strip()
                    tail = p[i_rel + len("<rel>"): i_obj].strip()
                    if head and rel and tail:
                        triples.append((head, rel, tail))
                    continue
                except Exception:
                    pass
            # å¸¸è§æ ¼å¼ï¼šåªå‡ºç° <subj> ä¸ <obj>ï¼Œå…³ç³»å‡ºç°åœ¨ <obj> ä¹‹å
            if ("<subj>" in p) and ("<obj>" in p):
                try:
                    i_sub = p.index("<subj>")
                    i_obj = p.index("<obj>")
                    head = p[:i_sub].strip()
                    tail = p[i_sub + len("<subj>"): i_obj].strip()
                    rel = p[i_obj + len("<obj>"):].strip()
                    # æ¸…ç†å¤šä½™ç©ºç™½
                    head = head.strip(" -:;,.\t\n")
                    tail = tail.strip(" -:;,.\t\n")
                    rel = rel.strip(" -:;,.\t\n")
                    # æ’é™¤æ˜æ˜¾æ— æ•ˆçš„å…³ç³»ï¼ˆç©ºã€ä»…æ•°å­—ç­‰ï¼‰
                    if head and tail and rel and not re.fullmatch(r"\d{1,4}", rel):
                        triples.append((head, rel, tail))
                        continue
                except Exception:
                    pass
            # <sep> åˆ†éš”
            if "<sep>" in p:
                fields = [x.strip() for x in p.split("<sep>") if x.strip()]
                if len(fields) >= 3:
                    head, rel, tail = fields[:3]
                    triples.append((head, rel, tail))
                    continue
            # å…œåº•ï¼šå°è¯•ç”¨æ‹¬å·æ ¼å¼æˆ–å¯å‘å¼
            # åç»­ä¼šç»Ÿä¸€å¤„ç†

    # æ‹¬å· (h, r, t)
    if "(" in s and ")" in s:
        for chunk in s.split("("):
            if ")" in chunk:
                body = chunk.split(")")[0]
                parts = [x.strip() for x in body.split(",")]
                if len(parts) == 3 and all(parts):
                    triples.append(tuple(parts))

    # å¦‚æœä»ä¸ºç©ºï¼Œå¯å‘å¼ï¼šæŒ‰åŒç©ºæ ¼åˆ‡å—ï¼Œæ»‘çª—å– (ent, rel, ent)
    if not triples:
        tokens = [t.strip() for t in s.split("  ") if t.strip()]
        # é‡‡ç”¨é•¿åº¦ä¸º3çš„æ»‘åŠ¨çª—å£ï¼Œè¦æ±‚ä¸­é—´å—åƒæ˜¯å…³ç³»çŸ­è¯­ï¼ˆåŒ…å«ç©ºæ ¼æˆ–å°å†™è¯ï¼‰
        for i in range(0, len(tokens) - 2):
            h, r, t = tokens[i], tokens[i + 1], tokens[i + 2]
            # ç®€å•çº¦æŸï¼šå…³ç³»é€šå¸¸éå…¨å¤§å†™ï¼Œä¸”åŒ…å«ç©ºæ ¼/ä¸ºå¤šè¯
            if r and (" " in r or r.islower()) and h and t:
                # æ’é™¤çœ‹èµ·æ¥åƒå¹´ä»½çš„ä¸­é—´å—
                if not re.fullmatch(r"\d{1,4}", r):
                    triples.append((h, r, t))
    return triples

# ================================
# 3.2ï¸âƒ£ ä½¿ç”¨ REBEL åŸç”Ÿæ¨¡å‹ç”Ÿæˆï¼ˆä¿ç•™ç‰¹æ®Štokenï¼‰
# ================================
@torch.no_grad()
def rebel_generate(text: str, max_length: int = 384, device: str = "cpu") -> str:
    """ç”¨ REBEL åŸç”Ÿæ¨¡å‹ç”Ÿæˆï¼Œç¦ç”¨ skip_special_tokens ä»¥ä¿ç•™æ ‡è®°ï¼Œä¾¿äºè§£æã€‚"""
    if rebel_model is None or rebel_tok is None:
        init_worker()
    inputs = rebel_tok(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    )
    if device == "cuda" and torch.cuda.is_available():
        model_device = torch.device("cuda")
    else:
        model_device = torch.device("cpu")
    inputs = {k: v.to(model_device) for k, v in inputs.items()}
    rebel_model.to(model_device)
    outputs = rebel_model.generate(**inputs, max_length=max_length)
    decoded = rebel_tok.batch_decode(outputs, skip_special_tokens=False)[0]
    return decoded
# ================================
# 4ï¸âƒ£ REBEL + æ—¶é—´æŠ½å–å‡½æ•° (åŒ…å«è§£ç é€»è¾‘)
# ================================
def extract_relations_with_time(example):
    # ç¡®ä¿æ¨¡å‹å·²åœ¨å½“å‰è¿›ç¨‹ä¸­åˆå§‹åŒ–
    init_worker()

    text = ""
    # æ£€æŸ¥å¹¶è§£ç  input_ids
    if "input_ids" in example:
        text = tokenizer.decode(example["input_ids"], skip_special_tokens=True)
    elif "text" in example: # ä¿ç•™å¯¹çº¯æ–‡æœ¬æ ¼å¼çš„å…¼å®¹
        text = example["text"]
    
    if not text.strip():
        return {"quadruples": []}

    try:
        # æˆªå–éƒ¨åˆ†æ–‡æœ¬ä»¥æé«˜æ•ˆç‡
        text_snippet = text[:512]
        # ä½¿ç”¨åŸç”Ÿæ¨¡å‹ç”Ÿæˆï¼Œä¿ç•™ç‰¹æ®Štoken
        generated_text = rebel_generate(text_snippet, max_length=384)

        # è§£æç”Ÿæˆç»“æœä¸º (h, r, t)
        triples = parse_rebel_output(generated_text)

        # æå–æ—¶é—´å¹¶ç»„è£…å››å…ƒç»„
        times = extract_time_expressions(text_snippet)
        quadruples = [(h, r, t, times[0]) for (h, r, t) in triples]

        # å¯é€‰ï¼šè°ƒè¯•å‰å‡ æ¡
        if not hasattr(extract_relations_with_time, "_dbg"): 
            extract_relations_with_time._dbg = 0
        extract_relations_with_time._dbg += 1
        if extract_relations_with_time._dbg <= 3:
            print("\nğŸ” DEBUG sample:")
            print("Text:", text_snippet[:120].replace("\n", " "), "...")
            print("Generated:", generated_text[:200].replace("\n", " "), "...")
            print("Triples:", triples[:3])
            print("Times:", times[:3])

        return {"quadruples": quadruples}
    except Exception as e:
        # æ‰“å°é”™è¯¯ä»¥ä¾¿è°ƒè¯•
        print(f"Error in process {os.getpid()}: {e}")
        return {"quadruples": [], "error": str(e)}

# ================================
# 5ï¸âƒ£ å†…å­˜æ£€æµ‹å‡½æ•°
# ================================
def wait_for_memory(threshold_gb=2):
    while True:
        free_mem = psutil.virtual_memory().available / (1024**3)
        if free_mem < threshold_gb:
            print(f"âš ï¸ Low memory ({free_mem:.2f} GB free). Waiting 10s...")
            time.sleep(10)
        else:
            break

# ================================
# 6ï¸âƒ£ ä¸»æ‰¹å¤„ç†é€»è¾‘
# ================================
if __name__ == "__main__":
    arrow_files = sorted([os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".arrow")])
    print(f"ğŸ”¹ Found {len(arrow_files)} Arrow files.")

    # æœ¬è½®ç›®æ ‡ï¼šå¯¹ 17..91 å·åˆ†ç‰‡ï¼Œåœ¨å·²æœ‰ ~50 æ ·æœ¬çš„åŸºç¡€ä¸Šï¼Œè¿½åŠ æ ·æœ¬ä½¿æ¯ä¸ªCSVè¾¾åˆ° 200 è¡Œ
    UPDATE_START, UPDATE_END = 17, 91  # åŒ…å«ç«¯ç‚¹
    TARGET_SAMPLES = 200
    BASE_OFFSET = 50                  # æœŸæœ›åœ¨å·²æœ‰50åŸºç¡€ä¸Šè¿½åŠ 

    # é…ç½®ï¼šå¤„ç†è§„æ¨¡ä¸å†™ç›˜ç­–ç•¥ï¼ˆä¸å‹æ¦¨æ€§èƒ½ï¼‰
    NUM_PROC = 4                 # å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
    FLUSH_EVERY_N = 20000        # ç´¯ç§¯åˆ°Nè¡Œå³è¿½åŠ å†™ç›˜ï¼Œé¿å…å å†…å­˜

    def csv_has_rows(path: str) -> bool:
        try:
            if not os.path.exists(path):
                return False
            df_head = pd.read_csv(path, nrows=1)
            return len(df_head) > 0
        except Exception:
            return False

    def csv_row_count(path: str) -> int:
        """è¯»å–CSVè¡Œæ•°ï¼ˆä¸å«è¡¨å¤´ï¼‰ã€‚ä¸å­˜åœ¨åˆ™ä¸º0ã€‚"""
        if not os.path.exists(path):
            return 0
        try:
            with open(path, "r", encoding="utf-8") as f:
                return max(0, sum(1 for _ in f) - 1)
        except Exception:
            return 0

    for i, file in enumerate(arrow_files):
        base_name = os.path.basename(file).replace(".arrow", "")
        output_file = os.path.join(OUTPUT_DIR, f"{base_name}_quads.csv")

        # è§£æåˆ†ç‰‡ç¼–å·ï¼ˆä¾‹å¦‚ data-00020-of-00092 -> 20ï¼‰
        try:
            shard_idx = int(base_name.split("-")[1])
        except Exception:
            print(f"âš ï¸  Cannot parse shard index from {base_name}, skipping.")
            continue

        # ä»…å¤„ç† 17..91 èŒƒå›´
        if not (UPDATE_START <= shard_idx <= UPDATE_END):
            continue

        # è®¡ç®—å½“å‰CSVå·²æœ‰è¡Œæ•°ï¼Œå†³å®šè¿½åŠ åŒºé—´
        existing_rows = csv_row_count(output_file)
        if existing_rows >= TARGET_SAMPLES:
            print(f"âœ… Skipping {base_name} (already has {existing_rows} rows â‰¥ {TARGET_SAMPLES}).")
            continue
        # ä»50å¼€å§‹è¿½åŠ ï¼Œå¦‚æœä¸è¶³50åˆ™ä»å·²æœ‰è¡Œæ•°å¼€å§‹ï¼Œç›®æ ‡åˆ°200
        start_idx = BASE_OFFSET if existing_rows >= BASE_OFFSET else existing_rows
        end_idx = TARGET_SAMPLES
        print(f"\nProcessing shard {shard_idx}: {base_name} -> append [{start_idx}:{end_idx}) (current={existing_rows})")
        wait_for_memory(4) # å»ºè®®ä¸ºå¤šè¿›ç¨‹ç•™å‡ºæ›´å¤šå†…å­˜

        try:
            # ä½¿ç”¨æµå¼å¤„ç†ï¼Œä¸å°†æ•°æ®å®Œå…¨åŠ è½½åˆ°å†…å­˜
            dataset = load_dataset("arrow", data_files=file, cache_dir=None, keep_in_memory=False)["train"]

            total = len(dataset)
            if start_idx >= total:
                print(f"â„¹ï¸  start_idx {start_idx} exceeds total {total}, skip {base_name}")
                continue
            end_eff = min(end_idx, total)
            if start_idx >= end_eff:
                print(f"â„¹ï¸  Nothing to do for {base_name} (start {start_idx} >= end {end_eff}).")
                continue
            dataset = dataset.select(range(start_idx, end_eff))
            print(f"ğŸ“Š Appending samples [{start_idx}:{end_eff}) out of {total}")

            dataset_with_quads = dataset.map(
                extract_relations_with_time,
                num_proc=NUM_PROC,
                batched=False
            )

            # å¢é‡å†™ç›˜
            rows = []
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰ä»»æ„å†…å®¹ï¼Œåˆ™ä¸å†å†™è¡¨å¤´
            wrote_header = os.path.exists(output_file) and os.path.getsize(output_file) > 0
            for example in tqdm(dataset_with_quads, desc=f"Saving {base_name} (append)"):
                for quad in example["quadruples"]:
                    if len(quad) == 4:
                        rows.append({
                            "head": quad[0],
                            "relation": quad[1],
                            "tail": quad[2],
                            "time": quad[3]
                        })
                if len(rows) >= FLUSH_EVERY_N:
                    df = pd.DataFrame(rows)
                    df.to_csv(output_file, mode="a", index=False, header=not wrote_header)
                    wrote_header = True
                    rows.clear()

            # flush æœ€åä¸€æ‰¹
            if rows:
                df = pd.DataFrame(rows)
                df.to_csv(output_file, mode="a", index=False, header=not wrote_header)
                rows.clear()

            # ç»Ÿè®¡è¡Œæ•°
            try:
                saved_rows = csv_row_count(output_file)
                print(f"âœ… Now {saved_rows} rows in {output_file}")
            except Exception:
                print(f"âœ… Saved to {output_file}")

        except Exception as e:
            print(f"âŒ Error processing {file}: {e}")
            continue

    print("\nğŸ‰ All Arrow files processed successfully!")