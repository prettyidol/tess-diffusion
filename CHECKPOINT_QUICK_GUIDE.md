# ðŸš€ Checkpoint ä¿å­˜é…ç½® - å¿«é€Ÿå‚è€ƒ

## âœ… å·²å®Œæˆçš„é…ç½®æ›´æ–°

### æ–°å¢žå‚æ•°

```json
"save_total_limit": 5
```

**ä½œç”¨**: æœ€å¤šä¿ç•™5ä¸ªcheckpointï¼Œè‡ªåŠ¨åˆ é™¤æœ€æ—§çš„

---

## ðŸ“¦ Checkpoint ä¿å­˜æ•ˆæžœ

### è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹

```
è®­ç»ƒå¼€å§‹...
â”œâ”€ Step 500   â†’ ä¿å­˜ checkpoint-500/   (åŒ…å«å®Œæ•´æ¨¡åž‹)
â”œâ”€ Step 1000  â†’ ä¿å­˜ checkpoint-1000/  (åŒ…å«å®Œæ•´æ¨¡åž‹)
â”œâ”€ Step 1500  â†’ ä¿å­˜ checkpoint-1500/  (åŒ…å«å®Œæ•´æ¨¡åž‹)
â”œâ”€ Step 2000  â†’ ä¿å­˜ checkpoint-2000/  (åŒ…å«å®Œæ•´æ¨¡åž‹)
â”œâ”€ Step 2500  â†’ ä¿å­˜ checkpoint-2500/  (åŒ…å«å®Œæ•´æ¨¡åž‹)
â””â”€ Step 3000  â†’ ä¿å­˜ checkpoint-3000/ + åˆ é™¤ checkpoint-500/ âœ…
```

### æ¯ä¸ª checkpoint åŒ…å«çš„æ–‡ä»¶

```
checkpoint-500/
â”œâ”€â”€ config.json           âœ… æ¨¡åž‹é…ç½®
â”œâ”€â”€ pytorch_model.bin     âœ… æ¨¡åž‹æƒé‡ (~500 MB)
â”œâ”€â”€ optimizer.pt          âœ… ä¼˜åŒ–å™¨çŠ¶æ€ (~1 GB)
â”œâ”€â”€ scheduler.pt          âœ… å­¦ä¹ çŽ‡è°ƒåº¦å™¨
â”œâ”€â”€ trainer_state.json    âœ… è®­ç»ƒè¿›åº¦
â”œâ”€â”€ training_args.bin     âœ… è®­ç»ƒå‚æ•°
â””â”€â”€ rng_state.pth         âœ… éšæœºæ•°çŠ¶æ€
```

**æ€»å¤§å°**: ~1.5 GB/checkpoint Ã— 5 = **~7.5 GB**

---

## ðŸ” é˜²ä¸¢å¤±ä¿æŠ¤æœºåˆ¶

### 1ï¸âƒ£ æ¯500æ­¥ä¿å­˜ checkpoint

```json
"save_steps": 500
```

**ä¿æŠ¤**: æœ€å¤šæŸå¤±500æ­¥è¿›åº¦ (~15-20åˆ†é’Ÿ)

### 2ï¸âƒ£ æ¯5åˆ†é’Ÿé¢å¤–å¿«ç…§

```json
"time_save_interval_seconds": 300
```

**ä¿æŠ¤**: æ—¶é—´è§¦å‘çš„é¢å¤–å¤‡ä»½

### 3ï¸âƒ£ è‡ªåŠ¨é™åˆ¶æ•°é‡

```json
"save_total_limit": 5
```

**ä¿æŠ¤**: é˜²æ­¢ç£ç›˜ç©ºé—´è€—å°½

---

## ðŸŽ¯ Google Colab ä½¿ç”¨æµç¨‹

### åœºæ™¯1: æ­£å¸¸è®­ç»ƒ (æ— ä¸­æ–­)

```bash
# ç›´æŽ¥å¼€å§‹è®­ç»ƒ
python run_mlm.py configs/tess_gpu_oneline_sc.json

# è®­ç»ƒè¿‡ç¨‹ä¼šè‡ªåŠ¨:
# âœ… æ¯500æ­¥ä¿å­˜checkpoint
# âœ… ä¿æŒæœ€æ–°5ä¸ªcheckpoint
# âœ… è‡ªåŠ¨åˆ é™¤æ—§checkpoint
```

### åœºæ™¯2: Colab æ–­çº¿åŽæ¢å¤

```bash
# 1. æŸ¥çœ‹çŽ°æœ‰checkpoint
ls -lh outputs/checkpoint-*

# è¾“å‡ºç¤ºä¾‹:
# checkpoint-1000/
# checkpoint-1500/
# checkpoint-2000/

# 2. ä»Žæœ€æ–°checkpointæ¢å¤
python run_mlm.py configs/tess_gpu_oneline_sc.json \
    --resume_from_checkpoint outputs/checkpoint-2000

# âœ… ä»Žç¬¬2000æ­¥ç»§ç»­è®­ç»ƒ
# âœ… ä¼˜åŒ–å™¨çŠ¶æ€å®Œå…¨æ¢å¤
# âœ… å­¦ä¹ çŽ‡è°ƒåº¦å™¨ç»§ç»­
```

### åœºæ™¯3: å¯ç”¨ Google Drive å¤‡ä»½ (æŽ¨è)

```python
# åœ¨ Colab Notebook ä¸­:

# 1. æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. ä¿®æ”¹é…ç½®å¯ç”¨å¤‡ä»½
import json
with open('configs/tess_gpu_oneline_sc.json', 'r') as f:
    config = json.load(f)

config['gdrive_backup_dir'] = '/content/drive/MyDrive/tess_backups'

with open('configs/tess_gpu_oneline_sc.json', 'w') as f:
    json.dump(config, f, indent=2)

# 3. å¼€å§‹è®­ç»ƒ
!python run_mlm.py configs/tess_gpu_oneline_sc.json
```

**æ•ˆæžœ**: æ¯ä¸ªcheckpointåŒæ—¶å¤‡ä»½åˆ°Google Drive âœ…

---

## ðŸ“Š æ—¶é—´ä¼°ç®— (T4 GPU)

### 1 Epoch (~10,000 æ ·æœ¬)

```
æ€»æ­¥æ•°: 625 æ­¥
è®­ç»ƒæ—¶é—´: ~2 å°æ—¶
ä¿å­˜checkpoint: checkpoint-500, checkpoint-625
ç£ç›˜ä½¿ç”¨: ~3 GB
```

### 3 Epochs

```
æ€»æ­¥æ•°: 1,875 æ­¥
è®­ç»ƒæ—¶é—´: ~6-7 å°æ—¶
ä¿å­˜checkpoint: 
  - checkpoint-500   (Epoch 1, 27%)
  - checkpoint-1000  (Epoch 2, 53%)
  - checkpoint-1500  (Epoch 3, 80%)
  - checkpoint-1875  (å®Œæˆ, 100%)
ç£ç›˜ä½¿ç”¨: ~6 GB
```

---

## ðŸ” å¸¸ç”¨å‘½ä»¤

### æŸ¥çœ‹æ‰€æœ‰ checkpoint

```bash
ls -lh outputs/ | grep checkpoint
```

### æŸ¥çœ‹ checkpoint è¯¦æƒ…

```bash
# æŸ¥çœ‹æŸä¸ªcheckpointåŒ…å«çš„æ–‡ä»¶
ls -lh outputs/checkpoint-1500/

# æŸ¥çœ‹è®­ç»ƒçŠ¶æ€
cat outputs/checkpoint-1500/trainer_state.json
```

### æ‰‹åŠ¨æ¸…ç† (å¦‚æžœéœ€è¦)

```bash
# åªä¿ç•™æœ€æ–°3ä¸ª
ls -dt outputs/checkpoint-* | tail -n +4 | xargs rm -rf

# åˆ é™¤ç‰¹å®šcheckpoint
rm -rf outputs/checkpoint-500
```

### ä»Žç‰¹å®šæ­¥æ•°æ¢å¤

```bash
# ä»Žcheckpoint-1000ç»§ç»­è®­ç»ƒ
python run_mlm.py configs/tess_gpu_oneline_sc.json \
    --resume_from_checkpoint outputs/checkpoint-1000
```

---

## âš ï¸ é‡è¦æç¤º

### 1. ç£ç›˜ç©ºé—´æ£€æŸ¥

```bash
# æ£€æŸ¥å¯ç”¨ç©ºé—´
df -h /content

# Colab é»˜è®¤: ~100 GB
# 5ä¸ªcheckpoint: ~7.5 GB
# å‰©ä½™ç©ºé—´: ~92 GB âœ…
```

### 2. è®­ç»ƒå®ŒæˆåŽ

æœ€ç»ˆæ¨¡åž‹ä¿å­˜åœ¨:
```
outputs/
â”œâ”€â”€ config.json            # æœ€ç»ˆé…ç½®
â”œâ”€â”€ pytorch_model.bin      # æœ€ç»ˆæƒé‡
â””â”€â”€ checkpoint-N/          # æœ€åŽä¸€æ­¥çš„å®Œæ•´çŠ¶æ€
```

### 3. å¦‚æžœç©ºé—´ä¸è¶³

ä¿®æ”¹é…ç½®å‡å°‘ä¿ç•™æ•°é‡:
```json
"save_total_limit": 3  // ä»Ž5æ”¹ä¸º3
```

---

## âœ… é…ç½®éªŒè¯æ¸…å•

- âœ… `save_strategy`: "steps" - æŒ‰æ­¥æ•°ä¿å­˜
- âœ… `save_steps`: 500 - æ¯500æ­¥ä¿å­˜
- âœ… `save_total_limit`: 5 - æœ€å¤š5ä¸ªcheckpoint
- âœ… `evaluation_strategy`: "steps" - åŒæ—¶è¯„æµ‹
- âœ… `eval_steps`: 500 - æ¯500æ­¥è¯„æµ‹
- âœ… `time_save_interval_seconds`: 300 - é¢å¤–æ—¶é—´å¤‡ä»½

**æ‰€æœ‰è®¾ç½®å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼** ðŸŽ‰

---

## ðŸ“ è®­ç»ƒæ—¥å¿—ç¤ºä¾‹

```
Training...
Step 500/1875  [=======>.......] 27%  Loss: 2.34
  âœ… Saving checkpoint to outputs/checkpoint-500
  âœ… Evaluation: MRR=0.28

Step 1000/1875 [=============>.] 53%  Loss: 1.89
  âœ… Saving checkpoint to outputs/checkpoint-1000
  âœ… Evaluation: MRR=0.32

Step 1500/1875 [==================>.] 80%  Loss: 1.56
  âœ… Saving checkpoint to outputs/checkpoint-1500
  âœ… Evaluation: MRR=0.36

Step 1875/1875 [====================] 100%  Loss: 1.42
  âœ… Saving checkpoint to outputs/checkpoint-1875
  âœ… Training complete!
  âœ… Final model saved to outputs/
```

